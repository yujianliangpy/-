# 第1章 统计学习及监督学习概论

## 1.1 统计学习

### 1.统计学习的特点

（1）统计学习以计算机和网络为平台；

（2）以数据为研究对象

（3）目的是对数据进行分析与预测

（4）以方法为中心

（5）交叉学科

### 2.统计学习的对象

以数据为对象，从数据中提取特征，抽象出模型，发现数据中的知识，回到数据的分析与预测

### 3.统计学习的目的

用于数据的预测和分析，特别是对未知新数据的预测和分析

### 4.统计学习的方法

统计学习的方法是基于数据构建概率统计模型对数据进行预测和分析，由监督学习、无监督学习和强化学习等组成

### 5.统计学习的研究

研究包括统计学习方法、统计学习理论及统计学习应用三个方面

## 1.2统计学习的分类

### 1.2.1 基本分类

1.监督学习

指从标注数据中学习预测模型的机器学习问题

实现步骤：

```
1.得到一个有限的训练数据集合
2.确定模型的假设空间，即所有的备选模型
3.确定模型选择的准则，即学习的策略
4.实现求解最优模型的算法
5.通过学习方法选择最优模型
6.利用最优模型对新数据进行预测或分析
```

2.无监督学习

从无标注数据中学习预测模型的机器学习问题

3.强化学习

指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题

4.半监督学习与主动学习

指利用少量标注数据和大量未标注数据学习预测模型的机器学习问题

### 1.2.2 按模型分类

1.概率模型与非概率模型

概率模型取条件概率分布形式 P(x|y)

非概率模型取函数形式 y=f(x)

2.线性模型与非线性模型

3.参数化模型与非参数化模型

### 1.2.3 按算法分类

在线学习：每次接受一个样本，进行预测，之后学习模型，不断重复

批量学习：一次接受所有数据，学习模型，之后进行预测

### 1.2.4 按技巧分类

1.贝叶斯学习

2.核方法

## 1.3 统计学习方法三要素

方法=模型+策略+算法

### 1.3.1 模型

在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。

### 1.3.2 策略

1.损失函数和风险函数

（1）0-1损失函数
$$
L(Y,f(X))=\begin{cases}
1, Y\neq f(X)\\
0, Y = f(X)
\end{cases}
$$
（2）平方损失函数
$$
L(Y,f(X))=(Y-f(X))^2
$$
（3）绝对损失函数
$$
L(Y,f(X))=|Y-f(X)|
$$
（4）对数损失函数
$$
L(Y,P(Y|X))=-logP(Y|X)
$$
2.经验风险最小化和结构风险最小化

经验风险最小化

结构风险最小化

### 1.3.3 算法

指学习模型的具体计算方法

## 1.4 模型评估与模型选择

### 1.4.1 训练误差与测试误差

训练误差是模型关于训练数据集的平均损失

测试误差是模型关于测试数据集的平均损失

### 1.4.2 过拟合与模型选择

若一味追求提高对于训练数据的预测能力，所选模型的复杂度则往往会比真模型更高，这种现象称为过拟合，指学习时选择的模型所包含的参数过多，出现此模型对于已知数据预测很好，对未知数据预测很差的现象。

模型选择旨在避免过拟合并提高模型的预测能力。

模型的复杂度增大时，训练误差会逐渐减小并趋于0，测试误差会先减小，达到最小值后又增大，当选择的模型复杂度过大时，会发生过拟合现象。

## 1.5 正则化与交叉验证

### 1.5.1 正则化

正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项

正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大

例如正则化项可以是模型参数向量的范数

### 1.5.2 交叉验证

训练集：模型的训练

验证集：模型的选择

测试集：模型的评估

1.简单交叉验证

随机将已有数据分为两部分，一部分为训练集，一部分为测试集，用训练集在各种条件下训练模型，得到不同的模型，在测试集上评价各个模型的测试误差，选出测试误差小的模型

2.S折交叉验证

随机将已有数据切分为S个互不相交、大小相同的子集，利用S-1个子集的数据训练模型，剩余的子集测试模型，将这一过程对可能的S种选择重复进行，最后选出S次评测中平均测试误差最小的模型

3.留一交叉验证

S折交叉验证的特殊情况是S=N，N为数据集的容量

## 1.6 泛化能力

### 1.6.1 泛化误差

泛化能力指由该学习方法学习到的模型对于未知数据的预测能力

模型对于未知数据预测的误差为泛化误差

### 1.6.2 泛化误差上界

学习方法的泛化能力分析是通过比较两种学习方法的泛化误差上界的大小来比较他们的优劣

## 1.7 生成模型与判别模型

生成方法：模型表示了给定输入X产生输出Y的生成关系

判别方法：给定的输入X，预测什么样的输出Y

## 1.8 监督学习应用

### 1.8.1 分类问题

输出变量Y取有限个离散值时，预测问题便成为分类问题

分离器：监督学习从数据中学习一个分类模型或分类决策函数

分类：分类器对新的输入进行输出的预测

<img src="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20220514154352775.png" alt="image-20220514154352775" style="zoom: 33%;" />

对于二分类问题常见的评价指标是精确率和召回率

TP--将正类预测为正类的数量；

FN--将正类预测为负类的数量；

FP--将负类预测为正类的数量；

TN--将负类预测为负类的数量；

精确率：
$$
P=\frac{TP}{TP+FP}
$$
召回率：
$$
R=\frac{TP}{TP+FN}
$$
F1值：
$$
\frac{2}{F1}=\frac{1}{P}+\frac{1}{R}
$$

$$
F1=\frac{2TP}{2TP+FP+FN}
$$

### 1.8.2 标注问题

标注：tagging  结构预测：structure prediction

输入：观测序列

输出：标记序列或状态序列

训练集：
$$
T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}
$$
观测序列：
$$
x_i=(x_i^1,x_i^2,\cdots,x_i^n)^T,i=1,2,\cdots,N
$$
输出标记：
$$
y_i=(y_i^1,y_i^2,\cdots,y_i^n)^T
$$


### 1.8.3 回归问题

回归用于预测输入变量和输出变量之间的关系，分为学习和预测两个阶段

训练集：
$$
T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}
$$
<img src="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20220514161122628.png" alt="image-20220514161122628" style="zoom: 33%;" />

# 第2章 感知机

感知机（perceptron）是二分类的线性分类模型，输入为实例的特征向量，输出为实例的类别，取+1和-1；

感知机对于输入空间中将实例划分为正负两类的分离超平面，属于判别模型；

 导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化；

简单易实现，分为原始形式和对偶形式；

## 2.1 感知机模型

$$
f(x)=sign(w\cdot x+b)
$$

输入x表示实例的特征向量，y表示实例的类别，w为权值或权值向量（weight / weight vector），b为偏置（bias）

w*x 表示w和x的内积，sign是符号函数
$$
sign(x)=\begin{cases} +1,x\geq0 \\ -1,x\lt0 \end{cases}
$$
感知机的几何解释

线性方程
$$
w\cdot x+b=0
$$
对应于特征空间R中的一个超平面S，w为超平面的法向量，b是截距，此超平面将特征空间分为两个部分，位于两部分的点（特征向量）分别被分为正负两类，超平面S称为分离超平面（separating hyperplane）

<img src="%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.assets/image-20220523222350235.png" alt="image-20220523222350235" style="zoom:33%;" />

## 2.2 感知机学习策略

数据集的线性可分性：对于一个数据集若存在某个超平面S能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，则称该数据集为线性可分数据集。

